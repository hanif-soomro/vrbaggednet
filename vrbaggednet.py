# -*- coding: utf-8 -*-
"""VRBaggedNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p25MmByNjCeCaT3jnFfouAoDU5rfnUVB
"""



"""#**VRBagged-Net**
This notebook implements VRBagged-Net framework, which utilizes the power of deep learning based pre-trained models along with bagging technique, to produce effective results for classification of disaster events. 
"""

#Libariries 
import os
import numpy
import numpy as np
import scipy as scp
import scipy.misc
import glob
import numpy as np
import csv
import cv2 
import random 

from keras.models import *
from keras.layers import *
from keras.applications import *
from keras.preprocessing.image import *
from sklearn.model_selection import train_test_split

import numpy as np
from tqdm import tqdm
import cv2
import keras

from keras import optimizers
from keras.optimizers import SGD
from keras.optimizers import Adam

#Authentication from GoogleDrive 
from google.colab import drive 
drive.mount('/content/gdrive')

"""#**Data Augmentation**
Data augmentation technique is applied to increase the quantity of image in minority class. For this purpose, Augmentor library is utilized to create multiple augmented copies of same image. Number of images in minority class are increased to equalize the quantity of both classes. 

"""

!pip install Augmentor

#Code to generate single copy of each image in folder. This cell can be run for 9 time to generate 9 different samples of each image in class Flood related topics
import Augmentor 
p = Augmentor.Pipeline("/content/gdrive/My Drive/MediaEval_2019/Dataset/Train_5181/1") #Path of directory, which stores images of class 1 "Flood related topic"

#Different parameters for augmentation
p.rotate(probability=1, max_left_rotation=20, max_right_rotation=20)
p.flip_left_right(probability=0.5)
p.flip_top_bottom(probability=0.2)
p.zoom(probability=0.5, min_factor=1.1, max_factor=1.5)
p.process()



"""#**Selection of Stratified Random Sample**
Stratified random sample images with same quantity (i-e 3000) is selected from each class. 
"""

#SELECTION OF RANDOM SAMPLES FROM EACH CLASS 

path_00 ='/content/gdrive/My Drive/MediaEval_2019/Dataset/Train_5181/0'         #Path of directory, which stores images of class 0 or No-Flood related topic
path_01 ='/content/gdrive/My Drive/MediaEval_2019/Dataset/Train_5181/1'         #Path of directory, which stores images of class 1 or Flood related topic

names_00 = os.listdir(path_00)                                                  #os.listdir return number of files in directory of No-Flood related topic
names_01 = os.listdir(path_01)                                                  #os.listdir return number of files in directory of Flood related topic

random_00=random.sample(population=names_00, k=3000)                            #selection of 3000 sample from total population of class No-flood related topic. 
random_01=random.sample(population=names_01, k=3000)                            #selection of 3000 sample from total population of class Flood related topic.

print(random_00)                                                                #represent image id of individual image
os.path.join(path_00, random_00[1])                                             #represent complete path for individual image

"""An Stratified random sample (i-e 3000) is selected from class 0 (No-Flood related Topic)."""

#LOAD IMAGES OF CLASS 'NO-FLOOD RELATED TOPIC'

n = 6000                                                                        #n represent total sample size of population
X = np.zeros((n, 224, 224, 3), dtype=np.uint8)
y = np.zeros((n, 1), dtype=np.uint8)

print("Loading images of class 'NoFlood related topic")
print(random_00)                                                              

internal_counter = 0                                                            #Counter for image to be loaded for single class 'No-flood related topic'
external_counter = 0                                                            #Counter for all images to be load from both classes 

for img_path in tqdm(random_00):                                                #iterate to fetch each file from 3000 images for class "No-flood related topic"
    path_randomImages=os.path.join(path_00, random_00[internal_counter])        #append directory of 'No-flood related topic' class images with ids of randomly selected images 
    X[external_counter] = cv2.resize(cv2.imread(path_randomImages), (224, 224)) #The X[external_counter] will load and resize images for class "No-flood related topic"
    y[external_counter] = 0                                                     #y[external_counter] will assign 0 for images of class "No-flood related topic"    
    external_counter+=1                                                         #external counter is increased by one after loading of each image 
    internal_counter+=1                                                         #internal counter is increased by one after loading of each image

print ("Loading of images of class 'No-Flood related topic' is complete")

"""An Stratified random sample (i-e 3000) is selected from class 1 (Flood related Topic)."""

#LOAD IMAGES OF CLASS 'FLOOD RELATED TOPIC'

print("Loading images of class 'Flood related topic")
internal_counter = 0                                                            #Counter for image to be loaded for single class 'Flood related topic' 
for img_path in tqdm(random_01):                                                #iterate to fetch each file from 3000 images for class "Flood related topic" 
    path_randomImages=os.path.join(path_01, random_01[internal_counter])        #append directory of 'Flood related topic' class images with ids of randomly selected images 
    X[external_counter] = cv2.resize(cv2.imread(path_randomImages), (224, 224)) #The X[external_counter] will load and resize images for class "Flood related topic"  
    y[external_counter] = 1                                                     #y[external_counter] will assign 1 for images of class "Flood related topic"    
    external_counter+=1                                                         #external counter is increased by one after loading of each image 
    internal_counter+=1                                                         #internal counter is increased by one after loading of each image
print ("Loading of images of class 'Flood related topic' is complete")

"""**Train and Validation Split**
A quantity of 80% is allocated for training and remaining 20% is allocated for validation. 
"""

#20% OF SELECTED IMAGES ARE ALLOCATED TO VALIDATION SET AND REMAINING 80% ARE USED FOR TRAINING 
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2,  stratify=y)
print(X_train.shape)
print(X_valid.shape)
print(y_train.shape)
print(y_valid.shape)

!pip install update keras_applications

"""#**Loading Pre-trained Model**
VGG16 (Pre-trained on ImageNet and Places365) is loaded. 
"""

#LOADING MODEL 
#ls '/content/gdrive/My Drive/MediaEval_2017/Bagging_vgg16Hybrid/Keras-VGG16-places365-master/' 
import sys
sys.path.append('/content/gdrive/My Drive/MediaEval_2019/Keras-VGG16-places365-master/')                        #Loading already saved model 

from vgg16_hybrid_places_1365 import VGG16_Hybrid_1365                                                          #VGG16 Hybrid is pretrained on Imagenet and places365 datasets
base_model = VGG16_Hybrid_1365(weights='places', include_top=True)

"""#**Parameter Setting and Model compilation**
Parameters are set for the retraining of VGG16 and model is compiled with set parameters.
"""

#PARAMETER SETTING FOR FINETUNNING OF MODEL
x=base_model.output
x = Dropout(0.2)(base_model.output)
x = BatchNormalization()(x, training=False)
pred = Dense(1, activation='sigmoid')(x)
model=Model(inputs=base_model.input,outputs=pred)

opt = keras.optimizers.Adam(lr=0.00001)                                       
#opt = keras.optimizers.Adam(lr=0.00001)                                        

#Unfreeze last few layers to retrain the model 
for layer in model.layers[:-4]:
    layer.trainable = False
 
model.compile(loss='binary_crossentropy',optimizer=opt,metrics=['accuracy'])

"""#**VGGHybrid: Retraining of Model**"""

model.fit(X_train, y_train, batch_size=16, epochs=40, validation_data=(X_valid, y_valid), shuffle=True)

"""#**VGGHybrid: Prediction on Test Data**
Trained model is tested on unseen images. The unique id for the image of each tweet is saved in comma separated file, along with its prediction confidence. Moreover, the threshold of 0.5 is used for the prediction of either 1=Flood related top, or 0=No-flood related top class. 

"""

############LABEL PREDICTION FOR TEST-SET##########################

import os
import numpy
import numpy as np
import scipy as scp
import scipy.misc
import glob
import numpy as np
import csv
import cv2
import pandas as pd

from keras.models import load_model,Model

count = 0
arr_id=[]
for img_name in glob.glob('/content/gdrive/My Drive/MediaEval_2019/Dataset/TestSet_1296/*.*'):          #Directory which stores text-set images 
    image_resized = cv2.resize(cv2.imread(img_name), (224, 224)).astype(np.float32)                     #Loading and resizing images 
    image = image_resized.reshape((1, 224, 224, 3))                                         
    prob=model.predict(image)                                                                           #Getting confidence/probabilitic outcome of model for individual image  
    y_class = 0 if prob<0.5 else 1                                                                      #Threshold of 0.5 for prediction of 0=No-flood related or 1=Flood related class 
    
    #extract img_ids for each test-set image
    base=os.path.basename(img_name)
    os.path.splitext(img_name)
    img_name2 = os.path.splitext(base)[0]
    #counter for test-set 
    count+=1
    print(count)

    #Combine img_id, its respective prediction and probabilitic outcome
    d = {'img_id': [img_name2],'pred':y_class,'prob': str(prob).strip("[]") }
    df_pred = pd.DataFrame(data=d)  

    #Save outcome in csv file    
    print(df_pred)
    df_pred.to_csv('/content/gdrive/My Drive/MediaEval_2019/dir_VRoutcome_1/V_outcome_1.csv', mode='a', header=False, index=False)

"""#**ResNet50 Mode**

**Loading Pretrained Model**
"""

base_model_resnet = ResNet50(weights='imagenet',include_top=True,input_shape=(224, 224, 3))

"""**ResNet50: Parameter setting**"""

x=base_model_resnet.output
x = Dropout(0.3)(base_model_resnet.output)
x = BatchNormalization()(x, training=False)
pred = Dense(1, activation='sigmoid')(x)
model=Model(inputs=base_model_resnet.input,outputs=pred)

opt = keras.optimizers.Adam(lr=0.00001)

for layer in model.layers[:-10]:
   layer.trainable = False
 
model.compile(loss='binary_crossentropy',optimizer=opt,metrics=['accuracy'])

"""#**Training of ResNet50**"""

model.fit(X_train, y_train, batch_size=10, epochs=50, validation_data=(X_valid, y_valid), shuffle=True)

"""#**Prediction by ResNet50 based Model**"""

############LABEL PREDICTION FOR TEST-SET##########################

import os
import numpy
import numpy as np
import scipy as scp
import scipy.misc
import glob
import numpy as np
import csv
import cv2
import pandas as pd

from keras.models import load_model,Model

count = 0
arr_id=[]
for img_name in glob.glob('/content/gdrive/My Drive/MediaEval_2019/Dataset/TestSet_1296/*.*'):          #Directory which stores text-set images 
    image_resized = cv2.resize(cv2.imread(img_name), (224, 224)).astype(np.float32)                     #Loading and resizing images 
    image = image_resized.reshape((1, 224, 224, 3))                                         
    prob=model.predict(image)                                                                           #Getting confidence/probabilitic outcome of model for individual image  
    #y_class = 1 if prob>0.5 else 0                                                                        
    y_class = 0 if prob<0.5 else 1                                                                      #Threshold of 0.5 for prediction of 0=No-flood related or 1=Flood related class 
    
    #extract img_ids for each test-set image
    base=os.path.basename(img_name)
    os.path.splitext(img_name)
    img_name2 = os.path.splitext(base)[0]

    #counter for test-set 
    count+=1
    print(count)
    #Combine img_id, its respective prediction and probabilitic outcome
    d = {'img_id': [img_name2],'pred':y_class,'prob': str(prob).strip("[]") }

    df_pred = pd.DataFrame(data=d)  

    #Save outcome in csv file    
    print(df_pred)
    df_pred.to_csv('/content/gdrive/My Drive/MediaEval_2019/dir_VRoutcome_1/R_outcome_1.csv', mode='a', header=False, index=False)





